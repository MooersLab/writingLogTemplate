\newglossaryentry{adaptive design}{
    name={adaptive design},
    description={See Hybrid Monte Carlo.}
    }
\newglossaryentry{Approximate Bayesian Computation}{
    name={Approximate Bayesian Computation},
    description={A class of computational methods rooted in Bayesian statistics. ABC methods bypass the evaluation of the likelihood function and widen the realm of models for which statistical inference can be considered. ABC methods make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection.}
    }
\newglossaryentry{autocorrelation time}{
    name={autocorrelation time},
    description={The number of observations from an MCMC simulation that is equivalent to a single independent observation. It is also called the correlation length, and is often represented by $\tau$. Estimates based on a MCMC simulation with a correlation time of $\tau$ will have standard errors that are about the same as those from an independent sample that is smaller in size by a factor of  $\tau$.}
    }
\newglossaryentry{candidate kernel}{
    name={candidate kernel},
    description={The Metropolis-Hastings algorithm associated with a target density $\pi$ requires the choice a proposal kernel or candidate kernel or a conditional density from the transition of the Markov chain, $X^(t)$, from time, $t$, and its value, $X^(t+1)$,  at time $(t+1)$.}
    }
\newglossaryentry{censored data}{
    name={censored data},
    description={Censoring hides values from points that are too large, too small, or both. The number of data points that were censored is known, unlike the case for truncated data. Data are right-censored if the value is greater than a threshold. The data are left-censored if the value is below a threshold. The censored data can be treated as missing data. In Stan, the censored data have their own array and their mean and sigma are sampled.}
    }
\newglossaryentry{correlation length}{
    name={correlation length},
    description={See autocorrelation time.}
    }
\newglossaryentry{curse of dimensionality}{
    name={curse of dimensionality},
    description={The various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions). These phenomena do not occur in the analysis of  3-D data.  The problem is caused by the exponential increase in volume associated with adding extra dimensions to a (mathematical) space. The term was coined by the applied mathematician Richard Ernest Bellman (August 26, 1920 – March 19, 1984) who introduced dynamic programming in 1953.}
    } 
\newglossaryentry{diminishing adaptation condition}{
    name={diminishing adaptation condition},
    description={The distance between two consecutive Markov kernels must uniformly decrease to zero.}
    }
\newglossaryentry{effective sample size}{
    name={effective sample size},
    description={With regards to MCMC, this is the correction factor, $\tau_T$, such that $\sigma^2_T / \tau_T$ is the variance of the empirical average.}
    } 
\newglossaryentry{equidispersion}{
    name={equidispersion},
    description={Equal dispersion of parameter values.}
    }
\newglossaryentry{extra-dispersion}{
    name={extra-dispersion},
    description={Extra dispersion.}
    }
\newglossaryentry{Hybrid Monte Carlo}{
    name={Hybrid Monte Carlo},
    description={Also known as Hamiltonian Monte Carlo. A Markov chain Monte Carlo method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). It differs from the Metropolis-Hastings algorithm by reducing the correlation between successive sampled states by using a Hamiltonian evolution between states and additionally by targeting states with higher acceptance criteria than the observed probability distribution. This causes it to converge more quickly to the absolute probability distribution. It was devised by Simon Duane, A.D. Kennedy, Brian Pendleton, and Duncan Roweth in 1987.}
    }
\newglossaryentry{Hamiltonian Monte Carlo}{
    name={Hamiltonian Monte Carlo},
    description={See Hybrid Monte Carlo.}
    }
\newglossaryentry{irreducibility}{
    name={irreducibility},
    description={When a Markov chain has a stationary distribution and can eventually reach any region of the state space, no matter the initial value.}
    }
\newglossaryentry{leapfrog approximation}{
    name={leapfrog approximation},
    description={The Metropolis-Hastings correction required by the Hamiltonian Monte Carlo.}
    }
\newglossaryentry{Limited-memory Broyden–Fletcher–Goldfarb–Shanno}{
    name={Limited-memory Broyden–Fletcher–Goldfarb–Shanno},
    description={The L-BFGS is an optimization algorithm in the family of quasi-Newton methods. It approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning. The L-BFGS estimates the inverse Hessian matrix to steer its search through variable space. On the other hand, the BFGS stores a dense n×n approximation to the inverse Hessian, where n is the number of variables in the problem. L-BFGS stores only a few vectors that represent the approximation implicitly.}}
\newglossaryentry{Markov Chain Monte Carlo}{
    name={Markov Chain Monte Carlo},
    description={A class of algorithms that simulates a Markov chain whose stationary distribution is the target distribution of interest. The stationary chain generates a sample from the target distribution.}
    }
\newglossaryentry{Metropolis-within-Gibbs algorithm}{
    name={Metropolis-within-Gibbs algorithm},
    description={This algorithm aims to simulate a multidimensional distribution by successively simulating from some of the associated conditional distributions (the Gibbs part) and by using one Metropolis-Hastings steps.}
    }
\newglossaryentry{No U-turn sampler}{
    name={No U-turn sampler},
    description={An adaptive algorithm that aims to find the best parameter settings by tracking the sample path and preventing HMC from retracing its steps in this path.}
    }
\newglossaryentry{overdispersion}{
    name={overdispersion},
    description={When the observed variance exceeds the mean in count data.}
    }
\newglossaryentry{Poisson overdispersion}{
    name={Poisson overdispersion},
    description={The Poisson distribution has a mean that is equal to its variance. When the observed variance is greater than the mean; this is known as overdispersion and indicates that the Poisson model is inappropriate. A common reason for overdispersion is the omission of relevant explanatory variables or dependent observations. Under some circumstances, the problem of overdispersion can be solved by using quasi-likelihood estimation or a negative binomial distribution instead.}
    }
\newglossaryentry{probabilistic programming languages}{
    name={probabilistic programming languages},
    description={Probabilistic programming languages (PPLs) deploy functions that build Bayesian models and efficient automatic inference methods. PPLs separate model building from inference, thereby allowing users to focus on model building. The inference methods generate the posterior distribution, the posterior predictive distribution, and the prior predictive distribution. For Python, the PPL includes Pystan, PyMC, emcee, Pyro,  and Edward. I think Tensorflow Probability would fit in here, too, although it interfaces with neural networks.}
    }
\newglossaryentry{Riemannian manifold Hamiltonian Monte Carlo}{
    name={Riemannian manifold Hamiltonian Monte Carlo},
    description={aims to find the best parameter settings by providing adaptations using Riemannian geometry.}
    }
\newglossaryentry{simultaneous statistical inference}{
    name={simultaneous statistical inference},
    description={See Hybrid Monte Carlo.}
    }
\newglossaryentry{shock nucleation}{
    name={'shock' nucleation},
    description={Crystal nucleation due to the contact of highly concentrated protein stock solution with the precipitant stock solution before mixing is complete. Shock nucleation causes the formation of crystal nuclei in solutions that are really in the metastable region of the phase diagram.}
    }
\newglossaryentry{annotated bibliography}{
    name={annotated bibliography},
    description={An annotated bibliography gives a summary of each bibliographic entry. The annotation also provides the entry's central idea(s) and a general idea of the source's content. It also provides an evaluation of the source's credibility and its relevance to the field. It might contain an assessment of the relevance to own's own work when written for personal use. The following list of elements can be included. This is copied from \url{https://en.wikipedia.org/wiki/Annotated_bibliography}.
    \begin{itemize}
    \item \textbf{Full bibliographic citation:} the necessary and complete bibliographical information i.e. (author, title, publisher and date, etc.),
    \item \textbf{Author's background:} the name, authority, experience, or qualifications of the author.
    \item \textbf{Purpose of the work:} the reasons why the author wrote the work
    \item \textbf{Scope of the work:} the breadth or depth of coverage and topics or sub-topics covered.
    \item \textbf{Main argument:} State the main informative points of the paper
    \item \textbf{Audience:} For whom was it written (general public, subject specialists, student?
    \item \textbf{Methodology:} What methodology and research methods did the work employ?
    \item \textbf{Viewpoint:} What is the author's perspective or approach (school of thought, etc.)? E.g., an unacknowledged bias, or any undefended assumptions?
    \item \textbf{Sources:} Does the author cite other sources, and if so, what types? Is it based on the author's own research? Is it personal opinion?
    \item \textbf{Reliability of the source:} How reliable is the work?
    \item \textbf{Conclusion:} What does the author conclude about the work? Is the conclusion justified by the work?
    \item \textbf{Features:} Any significant extras, e.g. visual aids (charts, maps, etc.), reprints of source documents, an annotated bibliography?
    \item \textbf{Strengths and Weaknesses:} What are the strengths and weaknesses of the work?
    \item \textbf{Comparison:} How does the source relate to other works done by other writers on the topic: does it agree or disagree with another author or a particular school of thought; are there other works which would support or dispute it?
    \item \textbf{Voice / Personal Conclusion:} Provide an opinion of the work or a reaction to the source based on other available works, prior knowledge of the subject matter or knowledge pools done by other researchers.
    \end{itemize}
    }
}
